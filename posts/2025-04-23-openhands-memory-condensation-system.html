<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>"OpenHands' Memory and Condensation System for Code Understanding" - My Blog</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <h1>"OpenHands' Memory and Condensation System for Code Understanding"</h1>
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../about.html">About</a></li>
                <li><a href="../blog.html">Blog Posts</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <article>
            <p class="date">2025-04-23</p>
            <h1 id="openhands-memory-and-condensation-system-for-code-understanding">OpenHands&#39; Memory and Condensation System for Code Understanding</h1>
<h2 id="introduction">Introduction</h2>
<p>OpenHands takes a different approach to codebase understanding compared to other AI coding assistants. Instead of focusing heavily on preprocessing and indexing, OpenHands emphasizes efficient real-time search, chunking, and memory condensation. This unique approach enables OpenHands to work with large codebases while maintaining context efficiency.</p>
<h2 id="memory-and-condensation-the-core-innovation">Memory and Condensation: The Core Innovation</h2>
<p>The heart of OpenHands&#39; approach lies in its memory and condensation system, located in the <code>openhands/memory</code> directory. This system optimizes context window usage by intelligently summarizing chunks of conversation events:</p>
<pre><code class="language-python">class Condenser(ABC):
    &quot;&quot;&quot;Abstract condenser interface.
    
    Condensers take a list of `Event` objects and reduce them into a potentially smaller list.
    &quot;&quot;&quot;

    def __init__(self):
        self._metadata_batch: dict[str, Any] = {}
        self._llm_metadata: dict[str, Any] = {}
        
    @abstractmethod
    def condense(self, View) -&gt; View | Condensation:
        &quot;&quot;&quot;Condense a sequence of events into a potentially smaller list.&quot;&quot;&quot;
        
    def condensed_history(self, state: State) -&gt; View | Condensation:
        &quot;&quot;&quot;Condense the state&#39;s history.&quot;&quot;&quot;
        self._llm_metadata = state.to_llm_metadata(&#39;condenser&#39;)
        with self.metadata_batch(state):
            return self.condense(state.view)
</code></pre>
<h2 id="multiple-condenser-strategies-for-different-scenarios">Multiple Condenser Strategies for Different Scenarios</h2>
<p>OpenHands supports multiple condenser types, configured in <code>openhands/core/config/condenser_config.py</code>, including:</p>
<ol>
<li><strong>LLMSummarizingCondenser</strong>: Uses an LLM to create summaries of forgotten events</li>
<li><strong>AmortizedForgettingCondenser</strong>: Gradually forgets old events when history grows too large</li>
<li><strong>BrowserOutputCondenser</strong>: Specialized handler for browser output with masking capabilities</li>
<li><strong>ObservationMaskingCondenser</strong>: Masks older observations while maintaining an attention window</li>
</ol>
<p>A specialized <code>RollingCondenser</code> base class is provided for condensers that need to apply their logic to a rolling history:</p>
<pre><code class="language-python">class RollingCondenser(Condenser, ABC):
    &quot;&quot;&quot;Base class for a specialized condenser strategy that applies condensation to a rolling history.&quot;&quot;&quot;

    @abstractmethod
    def should_condense(self, view: View) -&gt; bool:
        &quot;&quot;&quot;Determine if a view should be condensed.&quot;&quot;&quot;

    @abstractmethod
    def get_condensation(self, view: View) -&gt; Condensation:
        &quot;&quot;&quot;Get the condensation from a view.&quot;&quot;&quot;

    def condense(self, view: View) -&gt; View | Condensation:
        if self.should_condense(view):
            return self.get_condensation(view)
        else:
            return view
</code></pre>
<h2 id="efficient-search-tools-ripgrep-wrappers">Efficient Search Tools: Ripgrep Wrappers</h2>
<p>For codebase search, OpenHands provides sophisticated tools exposed as function calls to the LLM/agent, including a <code>GrepTool</code> that wraps around ripgrep (<code>rg</code>) for fast regex-based code searching:</p>
<pre><code class="language-python">class GrepTool:
    &quot;&quot;&quot;Fast content search tool that works with any codebase size&quot;&quot;&quot;
    
    def __call__(self, pattern: str, path: str = None, include: str = None):
        &quot;&quot;&quot;
        Searches file contents using regular expressions
        
        Args:
            pattern: The regex pattern to search for
            path: Optional directory to search in
            include: Optional file pattern to include
            
        Returns:
            Matching file paths sorted by modification time
        &quot;&quot;&quot;
        cmd = [&quot;rg&quot;, &quot;--no-ignore&quot;, &quot;--hidden&quot;, &quot;--glob&quot;, &quot;!.git&quot;]
        
        if include:
            cmd.extend([&quot;--glob&quot;, include])
            
        cmd.extend([&quot;--json&quot;, pattern])
        
        if path:
            cmd.append(path)
</code></pre>
<p>OpenHands also includes a <code>GlobTool</code> that uses ripgrep&#39;s <code>--files</code> mode for fast file pattern matching:</p>
<pre><code class="language-python">class GlobTool:
    &quot;&quot;&quot;Fast file pattern matching tool&quot;&quot;&quot;
    
    def __call__(self, pattern: str, path: str = None):
        &quot;&quot;&quot;
        Find files matching a glob pattern
        
        Args:
            pattern: The glob pattern to match against
            path: Optional directory to search in
            
        Returns:
            List of matching file paths sorted by modification time
        &quot;&quot;&quot;
        cmd = [&quot;rg&quot;, &quot;--files&quot;, &quot;--no-ignore&quot;, &quot;--hidden&quot;, &quot;--glob&quot;, &quot;!.git&quot;, &quot;--glob&quot;, pattern]
        
        if path:
            cmd.append(path)
</code></pre>
<h2 id="chunk-localization-finding-relevant-code-segments">Chunk Localization: Finding Relevant Code Segments</h2>
<p>OpenHands implements a specialized chunk localization system to identify the most relevant chunks in a file:</p>
<pre><code class="language-python">def normalized_lcs(chunk: str, query: str) -&gt; float:
    &quot;&quot;&quot;Calculate the normalized Longest Common Subsequence (LCS) to compare file chunk with the query.

    We normalize Longest Common Subsequence (LCS) by the length of the chunk
    to check how **much** of the chunk is covered by the query.
    &quot;&quot;&quot;
    if len(chunk) == 0:
        return 0.0
    _score = pylcs.lcs_sequence_length(chunk, query)
    return _score / len(chunk)


def get_top_k_chunk_matches(
    text: str, query: str, k: int = 3, max_chunk_size: int = 100
) -&gt; list[Chunk]:
    &quot;&quot;&quot;Get the top k chunks in the text that match the query.&quot;&quot;&quot;
    raw_chunks = create_chunks(text, max_chunk_size)
    chunks_with_lcs: list[Chunk] = [
        Chunk(
            text=chunk.text,
            line_range=chunk.line_range,
            normalized_lcs=normalized_lcs(chunk.text, query),
        )
        for chunk in raw_chunks
    ]
    sorted_chunks = sorted(
        chunks_with_lcs,
        key=lambda x: x.normalized_lcs,
        reverse=True,
    )
    return sorted_chunks[:k]
</code></pre>
<p>The <code>Chunk</code> class organizes code into manageable pieces with line ranges:</p>
<pre><code class="language-python">class Chunk(BaseModel):
    text: str
    line_range: tuple[int, int]  # (start_line, end_line), 1-index, inclusive
    normalized_lcs: float | None = None

    def visualize(self) -&gt; str:
        lines = self.text.split(&#39;\n&#39;)
        assert len(lines) == self.line_range[1] - self.line_range[0] + 1
        ret = &#39;&#39;
        for i, line in enumerate(lines):
            ret += f&#39;{self.line_range[0] + i}|{line}\n&#39;
        return ret
</code></pre>
<h2 id="search-result-paging-and-iteration">Search Result Paging and Iteration</h2>
<p>OpenHands includes utilities for paginated search results using base64 encoding for page identifiers:</p>
<pre><code class="language-python">def offset_to_page_id(offset: int, has_next: bool) -&gt; str | None:
    if not has_next:
        return None
    next_page_id = base64.b64encode(str(offset).encode()).decode()
    return next_page_id


def page_id_to_offset(page_id: str | None) -&gt; int:
    if not page_id:
        return 0
    offset = int(base64.b64decode(page_id).decode())
    return offset


async def iterate(fn: Callable, **kwargs) -&gt; AsyncIterator:
    &quot;&quot;&quot;Iterate over paged result sets. Assumes that the results sets contain an array of result objects, and a next_page_id&quot;&quot;&quot;
    kwargs = {**kwargs}
    kwargs[&#39;page_id&#39;] = None
    while True:
        result_set = await fn(**kwargs)
        for result in result_set.results:
            yield result
        if result_set.next_page_id is None:
            return
        kwargs[&#39;page_id&#39;] = result_set.next_page_id
</code></pre>
<h2 id="practical-implementation-approach">Practical Implementation Approach</h2>
<p>When working with large codebases, OpenHands:</p>
<ol>
<li><p>Uses efficient command-line tools wrapped in safer interfaces (GrepTool, GlobTool)</p>
<ul>
<li>Ripgrep (<code>rg</code>) is the underlying tool for both search methods, providing fast performance even on large codebases</li>
<li>The wrappers handle complex patterns, escaping, and sanitize outputs for consumption by LLMs</li>
</ul>
</li>
<li><p>Employs memory condensation to optimize context usage</p>
<ul>
<li>Configurable strategies available through the <code>CondenserConfig</code> system</li>
<li>Automatic summarization of history when context grows too large</li>
<li>Maintaining only the most relevant context to conserve tokens</li>
</ul>
</li>
<li><p>Applies chunk localization to find relevant code segments</p>
<ul>
<li>Uses normalized Longest Common Subsequence (LCS) to measure relevance between code chunks and queries</li>
<li>Splits large files into manageable chunks for more effective processing</li>
<li>Preserves line numbers for accurate referencing</li>
</ul>
</li>
</ol>
<h2 id="computational-economics">Computational Economics</h2>
<p>OpenHands&#39; approach represents a balanced investment in real-time processing with good token efficiency:</p>
<ul>
<li><strong>Preprocessing Cost</strong>: Minimal - primarily real-time search and chunking</li>
<li><strong>Storage Overhead</strong>: Minimal - only stores condensed history</li>
<li><strong>Search Complexity</strong>: O(m) where m = search results (using ripgrep&#39;s optimized search)</li>
<li><strong>Token Efficiency</strong>: Good (optimizes through memory condensation)</li>
</ul>
<p>This approach is particularly well-suited for workflows with rapidly changing codebases where heavy preprocessing would quickly become stale.</p>
<h2 id="conclusion">Conclusion</h2>
<p>OpenHands&#39; memory and condensation system represents a lightweight yet effective approach to codebase understanding. By focusing on efficient real-time search, intelligent chunking, and sophisticated memory management, OpenHands achieves good performance while minimizing preprocessing overhead.</p>
<p>Key strengths of this approach include:</p>
<ul>
<li>Minimal preprocessing requirements</li>
<li>Adaptability to rapidly changing codebases</li>
<li>Efficient token usage through memory condensation</li>
<li>Flexible search capabilities with ripgrep integration</li>
<li>Chunk localization for finding relevant code segments</li>
</ul>
<p>This approach illustrates that effective codebase understanding doesn&#39;t always require heavyweight preprocessing and indexing - sometimes a more lightweight, real-time approach can be just as effective.</p>

        </article>
    </main>
    <footer>
        <p>&copy; 2025 My Blog. All rights reserved.</p>
    </footer>
</body>
</html>